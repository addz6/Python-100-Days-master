#-*-coding:utf-8-*-

import matplotlib.pyplot as plt
import numpy as np
import matplotlib

# Display plots inline and change default figure size
# %matplotlib inline
matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)

# Helper function to plot a decision boundary.
def plot_decision_boundary(pred_func, X, y):
    # Set min and max values and give it some padding
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole gid
    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.scatter(X[:, 0], X[:, 1], c=y.reshape((4, )), cmap=plt.cm.Spectral)


class NN(object):
    def __init__(self, n_x, n_h, n_y, learning_rate=0.01):
        """
        initialize the NN model

        parameters
        ----------
        n_x: int
            number of neurons in input layer
        n_h: int
            number of neurons in hidden layer
        n_y: int
            number of neurons in output layer
        learning_rate: float
            a hyperparam used in gradient descent
        """
        self.learning_rate = learning_rate
        self.W1 = np.random.randn(n_x, n_h)
        self.b1 = np.zeros(shape=(1, n_h))
        self.W2 = np.random.randn(n_h, n_y)
        self.b2 = np.zeros(shape=(1, n_y))

    def _sigmoid(self, Z):
        """a sigmoid activation function
        """
        return (1 / (1 + np.exp(-Z)))

    def feedforward(self, X):
        """performing the feedforward pass
        """
        # first hidden layer
        self.Z1 = np.dot(X, self.W1) + self.b1
        self.A1 = np.tanh(self.Z1)
        # second layer
        self.Z2 = np.dot(self.A1, self.W2) + self.b2
        self.A2 = self._sigmoid(self.Z2)
        return self.A2

    def compute_cost(self, A2, Y):
        """
        Computes the cross-entropy cost
        parameters
        ----------
        A2: np.ndarray
            the output generated by the output layer
        Y: np.ndarray
            the true labels

        return
        ----------
        cost: np.float64
            the cost per feedforward pass
        """
        m = Y.shape[0]  # number of example
        logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))
        cost = - np.sum(logprobs) / m
        cost = np.squeeze(cost)  # makes sure cost is the dimension we expect.
        return cost

    def backprop(self, X, Y):
        """
        performs the backpropagation algorithm

        parameters
        ------------
        X: np.ndarray
            representing the input that we feed to the neural net
        Y: np.ndarray
            the true label
        """
        m = X.shape[0]

        self.dZ2 = self.A2 - Y

        self.dW2 = (1. / m) * np.dot(self.A1.T, self.dZ2)
        self.db2 = (1. / m) * np.sum(self.dZ2, axis=0, keepdims=True)

        self.dZ1 = np.multiply(np.dot(self.dZ2, self.W2.T), 1 - np.power(self.A1, 2))
        self.dW1 = (1. / m) * np.dot(X.T, self.dZ1)
        self.db1 = (1. / m) * np.sum(self.dZ1, axis=0, keepdims=True)

    def update_parameters(self):
        """performs an update parameters for gradient descent
        """
        self.W1 -= self.learning_rate * self.dW1
        self.b1 -= self.learning_rate * self.db1
        self.W2 -= self.learning_rate * self.dW2
        self.b2 -= self.learning_rate * self.db2

    def predict(self, X):
        """
        an interface to generate prediction
        parameters
        ------------
        X: np.ndarray
           input features to our model

        return
        ------------
            np.ndarray - the predicted labels
        """
        A2 = self.feedforward(X)
        predictions = np.round(A2)
        return predictions

# data initialization
X = np.array([[0, 0],
                  [0, 1],
                  [1, 0],
                  [1, 1]])
y = np.array([[0,1,1,0]]).T

plt.scatter(X[:,0], X[:,1], s=40, c=y.reshape((4, )), cmap=plt.cm.Spectral)
plt.show()

def build_model(X, y, num_hidden, learning_rate=0.01, num_iterations=50000, verbose=True):
    """
    an intermediate method to train our model
    parameters
    -------------------
    X: numpy.ndarray
        input data
    y: numpy.ndarray
        the real label
    num_hidden: int
        number of hidden neurons in the layer
    learning_rate: float
        hyperparam for gradient descent algorithm
    num_iterations: int
        number of passes, each pass using number of examples.
    verbose: boolean
        optional. if True, it will print the cost per 1000 iteration

    return
    ---------------------
    model: an instance of NN object that represent the trained neural net model
    cost_history: a list containing the cost during training
    """
    model = NN(n_x=2, n_h=num_hidden, n_y=1, learning_rate=learning_rate)
    cost_history = []
    for i in range(0, num_iterations):
        A2 = model.feedforward(X)
        cost = model.compute_cost(A2, y)
        model.backprop(X, y)
        model.update_parameters()
        if i % 1000 == 0 and verbose:
            print("Iteration %i Cost: %f" % (i, cost))

        cost_history.append(cost)
    return model, cost_history

model, _ = build_model(X, y, 3, num_iterations=9500)

plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 3 neurons")
plt.show()

model, _ = build_model(X, y, 2) # 2 Neurons in our hidden layer

# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 2 neurons")
plt.show()

model, _ = build_model(X, y, 4) # 4 neurons in a hidden layer

# Plot the decision boundary
plot_decision_boundary(lambda x: model.predict(x), X, y)
plt.title("Neural Networks with 4 neurons")
plt.show()